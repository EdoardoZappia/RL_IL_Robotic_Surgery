{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from ConvAutoencoder import Encoder, Decoder\n",
    "from actor_critic_rnn import ActorCriticRNN, update, compute_reward\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import mujoco\n",
    "import glfw\n",
    "import numpy as np\n",
    "import time\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 0 terminated early at step 0.\n",
      "Episode 1/5000, Total Reward: 0.0000\n",
      "Episode 1 terminated early at step 0.\n",
      "Episode 2/5000, Total Reward: 0.0000\n",
      "Episode 2 terminated early at step 0.\n",
      "Episode 3/5000, Total Reward: 0.0000\n",
      "Episode 3 terminated early at step 0.\n",
      "Episode 4/5000, Total Reward: 0.0000\n",
      "Episode 4 terminated early at step 0.\n",
      "Episode 5/5000, Total Reward: 0.0000\n",
      "Episode 5 terminated early at step 0.\n",
      "Episode 6/5000, Total Reward: 0.0000\n",
      "Episode 6 terminated early at step 0.\n",
      "Episode 7/5000, Total Reward: 0.0000\n",
      "Episode 7 terminated early at step 0.\n",
      "Episode 8/5000, Total Reward: 0.0000\n",
      "Episode 8 terminated early at step 0.\n",
      "Episode 9/5000, Total Reward: 0.0000\n",
      "Episode 9 terminated early at step 0.\n",
      "Episode 10/5000, Total Reward: 0.0000\n",
      "Episode 10 terminated early at step 0.\n",
      "Episode 11/5000, Total Reward: 0.0000\n",
      "Episode 11 terminated early at step 0.\n",
      "Episode 12/5000, Total Reward: 0.0000\n",
      "Episode 12 terminated early at step 0.\n",
      "Episode 13/5000, Total Reward: 0.0000\n",
      "Episode 13 terminated early at step 0.\n",
      "Episode 14/5000, Total Reward: 0.0000\n",
      "Episode 14 terminated early at step 0.\n",
      "Episode 15/5000, Total Reward: 0.0000\n",
      "Episode 15 terminated early at step 0.\n",
      "Episode 16/5000, Total Reward: 0.0000\n",
      "Episode 16 terminated early at step 0.\n",
      "Episode 17/5000, Total Reward: 0.0000\n",
      "Episode 17 terminated early at step 0.\n",
      "Episode 18/5000, Total Reward: 0.0000\n",
      "Episode 18 terminated early at step 0.\n",
      "Episode 19/5000, Total Reward: 0.0000\n",
      "Episode 19 terminated early at step 0.\n",
      "Episode 20/5000, Total Reward: 0.0000\n",
      "Episode 20 terminated early at step 0.\n",
      "Episode 21/5000, Total Reward: 0.0000\n",
      "Episode 21 terminated early at step 0.\n",
      "Episode 22/5000, Total Reward: 0.0000\n",
      "Episode 22 terminated early at step 0.\n",
      "Episode 23/5000, Total Reward: 0.0000\n",
      "Episode 23 terminated early at step 0.\n",
      "Episode 24/5000, Total Reward: 0.0000\n",
      "Episode 24 terminated early at step 0.\n",
      "Episode 25/5000, Total Reward: 0.0000\n",
      "Episode 25 terminated early at step 0.\n",
      "Episode 26/5000, Total Reward: 0.0000\n",
      "Episode 26 terminated early at step 0.\n",
      "Episode 27/5000, Total Reward: 0.0000\n",
      "Episode 27 terminated early at step 0.\n",
      "Episode 28/5000, Total Reward: 0.0000\n",
      "Episode 28 terminated early at step 0.\n",
      "Episode 29/5000, Total Reward: 0.0000\n",
      "Episode 29 terminated early at step 0.\n",
      "Episode 30/5000, Total Reward: 0.0000\n",
      "Episode 30 terminated early at step 0.\n",
      "Episode 31/5000, Total Reward: 0.0000\n",
      "Episode 31 terminated early at step 0.\n",
      "Episode 32/5000, Total Reward: 0.0000\n",
      "Episode 32 terminated early at step 0.\n",
      "Episode 33/5000, Total Reward: 0.0000\n",
      "Episode 33 terminated early at step 0.\n",
      "Episode 34/5000, Total Reward: 0.0000\n",
      "Episode 34 terminated early at step 0.\n",
      "Episode 35/5000, Total Reward: 0.0000\n",
      "Episode 35 terminated early at step 0.\n",
      "Episode 36/5000, Total Reward: 0.0000\n",
      "Episode 36 terminated early at step 0.\n",
      "Episode 37/5000, Total Reward: 0.0000\n",
      "Episode 37 terminated early at step 0.\n",
      "Episode 38/5000, Total Reward: 0.0000\n",
      "Episode 38 terminated early at step 0.\n",
      "Episode 39/5000, Total Reward: 0.0000\n",
      "Episode 39 terminated early at step 0.\n",
      "Episode 40/5000, Total Reward: 0.0000\n",
      "Episode 40 terminated early at step 0.\n",
      "Episode 41/5000, Total Reward: 0.0000\n",
      "Episode 41 terminated early at step 0.\n",
      "Episode 42/5000, Total Reward: 0.0000\n",
      "Episode 42 terminated early at step 0.\n",
      "Episode 43/5000, Total Reward: 0.0000\n",
      "Episode 43 terminated early at step 0.\n",
      "Episode 44/5000, Total Reward: 0.0000\n",
      "Episode 44 terminated early at step 0.\n",
      "Episode 45/5000, Total Reward: 0.0000\n",
      "Episode 45 terminated early at step 0.\n",
      "Episode 46/5000, Total Reward: 0.0000\n",
      "Episode 46 terminated early at step 0.\n",
      "Episode 47/5000, Total Reward: 0.0000\n",
      "Episode 47 terminated early at step 0.\n",
      "Episode 48/5000, Total Reward: 0.0000\n",
      "Episode 48 terminated early at step 0.\n",
      "Episode 49/5000, Total Reward: 0.0000\n",
      "Episode 49 terminated early at step 0.\n",
      "Episode 50/5000, Total Reward: 0.0000\n",
      "Episode 50 terminated early at step 0.\n",
      "Episode 51/5000, Total Reward: 0.0000\n",
      "Episode 51 terminated early at step 0.\n",
      "Episode 52/5000, Total Reward: 0.0000\n",
      "Episode 52 terminated early at step 0.\n",
      "Episode 53/5000, Total Reward: 0.0000\n",
      "Episode 53 terminated early at step 0.\n",
      "Episode 54/5000, Total Reward: 0.0000\n",
      "Episode 54 terminated early at step 0.\n",
      "Episode 55/5000, Total Reward: 0.0000\n",
      "Episode 55 terminated early at step 0.\n",
      "Episode 56/5000, Total Reward: 0.0000\n",
      "Episode 56 terminated early at step 0.\n",
      "Episode 57/5000, Total Reward: 0.0000\n",
      "Episode 57 terminated early at step 0.\n",
      "Episode 58/5000, Total Reward: 0.0000\n",
      "Episode 58 terminated early at step 0.\n",
      "Episode 59/5000, Total Reward: 0.0000\n",
      "Episode 59 terminated early at step 0.\n",
      "Episode 60/5000, Total Reward: 0.0000\n",
      "Episode 60 terminated early at step 0.\n",
      "Episode 61/5000, Total Reward: 0.0000\n",
      "Episode 61 terminated early at step 0.\n",
      "Episode 62/5000, Total Reward: 0.0000\n",
      "Episode 62 terminated early at step 0.\n",
      "Episode 63/5000, Total Reward: 0.0000\n",
      "Episode 63 terminated early at step 0.\n",
      "Episode 64/5000, Total Reward: 0.0000\n",
      "Episode 64 terminated early at step 0.\n",
      "Episode 65/5000, Total Reward: 0.0000\n",
      "Episode 65 terminated early at step 0.\n",
      "Episode 66/5000, Total Reward: 0.0000\n",
      "Episode 66 terminated early at step 0.\n",
      "Episode 67/5000, Total Reward: 0.0000\n",
      "Episode 67 terminated early at step 0.\n",
      "Episode 68/5000, Total Reward: 0.0000\n",
      "Episode 68 terminated early at step 0.\n",
      "Episode 69/5000, Total Reward: 0.0000\n",
      "Episode 69 terminated early at step 0.\n",
      "Episode 70/5000, Total Reward: 0.0000\n",
      "Episode 70 terminated early at step 0.\n",
      "Episode 71/5000, Total Reward: 0.0000\n",
      "Episode 71 terminated early at step 0.\n",
      "Episode 72/5000, Total Reward: 0.0000\n",
      "Episode 72 terminated early at step 0.\n",
      "Episode 73/5000, Total Reward: 0.0000\n",
      "Episode 73 terminated early at step 0.\n",
      "Episode 74/5000, Total Reward: 0.0000\n",
      "Episode 74 terminated early at step 0.\n",
      "Episode 75/5000, Total Reward: 0.0000\n",
      "Episode 75 terminated early at step 0.\n",
      "Episode 76/5000, Total Reward: 0.0000\n",
      "Episode 76 terminated early at step 0.\n",
      "Episode 77/5000, Total Reward: 0.0000\n",
      "Episode 77 terminated early at step 0.\n",
      "Episode 78/5000, Total Reward: 0.0000\n",
      "Episode 78 terminated early at step 0.\n",
      "Episode 79/5000, Total Reward: 0.0000\n",
      "Episode 79 terminated early at step 0.\n",
      "Episode 80/5000, Total Reward: 0.0000\n",
      "Episode 80 terminated early at step 0.\n",
      "Episode 81/5000, Total Reward: 0.0000\n",
      "Episode 81 terminated early at step 0.\n",
      "Episode 82/5000, Total Reward: 0.0000\n",
      "Episode 82 terminated early at step 0.\n",
      "Episode 83/5000, Total Reward: 0.0000\n",
      "Episode 83 terminated early at step 0.\n",
      "Episode 84/5000, Total Reward: 0.0000\n",
      "Episode 84 terminated early at step 0.\n",
      "Episode 85/5000, Total Reward: 0.0000\n",
      "Episode 85 terminated early at step 0.\n",
      "Episode 86/5000, Total Reward: 0.0000\n",
      "Episode 86 terminated early at step 0.\n",
      "Episode 87/5000, Total Reward: 0.0000\n",
      "Episode 87 terminated early at step 0.\n",
      "Episode 88/5000, Total Reward: 0.0000\n",
      "Episode 88 terminated early at step 0.\n",
      "Episode 89/5000, Total Reward: 0.0000\n",
      "Episode 89 terminated early at step 0.\n",
      "Episode 90/5000, Total Reward: 0.0000\n",
      "Episode 90 terminated early at step 0.\n",
      "Episode 91/5000, Total Reward: 0.0000\n",
      "Episode 91 terminated early at step 0.\n",
      "Episode 92/5000, Total Reward: 0.0000\n",
      "Episode 92 terminated early at step 0.\n",
      "Episode 93/5000, Total Reward: 0.0000\n",
      "Episode 93 terminated early at step 0.\n",
      "Episode 94/5000, Total Reward: 0.0000\n",
      "Episode 94 terminated early at step 0.\n",
      "Episode 95/5000, Total Reward: 0.0000\n",
      "Episode 95 terminated early at step 0.\n",
      "Episode 96/5000, Total Reward: 0.0000\n",
      "Episode 96 terminated early at step 0.\n",
      "Episode 97/5000, Total Reward: 0.0000\n",
      "Episode 97 terminated early at step 0.\n",
      "Episode 98/5000, Total Reward: 0.0000\n",
      "Episode 98 terminated early at step 0.\n",
      "Episode 99/5000, Total Reward: 0.0000\n",
      "Episode 99 terminated early at step 0.\n",
      "Episode 100/5000, Total Reward: 0.0000\n",
      "Episode 100 terminated early at step 0.\n",
      "Episode 101/5000, Total Reward: 0.0000\n",
      "Episode 101 terminated early at step 0.\n",
      "Episode 102/5000, Total Reward: 0.0000\n",
      "Episode 102 terminated early at step 0.\n",
      "Episode 103/5000, Total Reward: 0.0000\n",
      "Episode 103 terminated early at step 0.\n",
      "Episode 104/5000, Total Reward: 0.0000\n",
      "Episode 104 terminated early at step 0.\n",
      "Episode 105/5000, Total Reward: 0.0000\n",
      "Episode 105 terminated early at step 0.\n",
      "Episode 106/5000, Total Reward: 0.0000\n",
      "Episode 106 terminated early at step 0.\n",
      "Episode 107/5000, Total Reward: 0.0000\n",
      "Episode 107 terminated early at step 0.\n",
      "Episode 108/5000, Total Reward: 0.0000\n",
      "Episode 108 terminated early at step 0.\n",
      "Episode 109/5000, Total Reward: 0.0000\n",
      "Episode 109 terminated early at step 0.\n",
      "Episode 110/5000, Total Reward: 0.0000\n",
      "Episode 110 terminated early at step 0.\n",
      "Episode 111/5000, Total Reward: 0.0000\n",
      "Episode 111 terminated early at step 0.\n",
      "Episode 112/5000, Total Reward: 0.0000\n",
      "Episode 112 terminated early at step 0.\n",
      "Episode 113/5000, Total Reward: 0.0000\n",
      "Episode 113 terminated early at step 0.\n",
      "Episode 114/5000, Total Reward: 0.0000\n",
      "Episode 114 terminated early at step 0.\n",
      "Episode 115/5000, Total Reward: 0.0000\n",
      "Episode 115 terminated early at step 0.\n",
      "Episode 116/5000, Total Reward: 0.0000\n",
      "Episode 116 terminated early at step 0.\n",
      "Episode 117/5000, Total Reward: 0.0000\n",
      "Episode 117 terminated early at step 0.\n",
      "Episode 118/5000, Total Reward: 0.0000\n",
      "Episode 118 terminated early at step 0.\n",
      "Episode 119/5000, Total Reward: 0.0000\n",
      "Episode 119 terminated early at step 0.\n",
      "Episode 120/5000, Total Reward: 0.0000\n",
      "Episode 120 terminated early at step 0.\n",
      "Episode 121/5000, Total Reward: 0.0000\n",
      "Episode 121 terminated early at step 0.\n",
      "Episode 122/5000, Total Reward: 0.0000\n",
      "Episode 122 terminated early at step 0.\n",
      "Episode 123/5000, Total Reward: 0.0000\n",
      "Episode 123 terminated early at step 0.\n",
      "Episode 124/5000, Total Reward: 0.0000\n",
      "Episode 124 terminated early at step 0.\n",
      "Episode 125/5000, Total Reward: 0.0000\n",
      "Episode 125 terminated early at step 0.\n",
      "Episode 126/5000, Total Reward: 0.0000\n",
      "Episode 126 terminated early at step 0.\n",
      "Episode 127/5000, Total Reward: 0.0000\n",
      "Episode 127 terminated early at step 0.\n",
      "Episode 128/5000, Total Reward: 0.0000\n",
      "Episode 128 terminated early at step 0.\n",
      "Episode 129/5000, Total Reward: 0.0000\n",
      "Episode 129 terminated early at step 0.\n",
      "Episode 130/5000, Total Reward: 0.0000\n",
      "Episode 130 terminated early at step 0.\n",
      "Episode 131/5000, Total Reward: 0.0000\n",
      "Episode 131 terminated early at step 0.\n",
      "Episode 132/5000, Total Reward: 0.0000\n",
      "Episode 132 terminated early at step 0.\n",
      "Episode 133/5000, Total Reward: 0.0000\n",
      "Episode 133 terminated early at step 0.\n",
      "Episode 134/5000, Total Reward: 0.0000\n",
      "Episode 134 terminated early at step 0.\n",
      "Episode 135/5000, Total Reward: 0.0000\n",
      "Episode 135 terminated early at step 0.\n",
      "Episode 136/5000, Total Reward: 0.0000\n",
      "Episode 136 terminated early at step 0.\n",
      "Episode 137/5000, Total Reward: 0.0000\n",
      "Episode 137 terminated early at step 0.\n",
      "Episode 138/5000, Total Reward: 0.0000\n",
      "Episode 138 terminated early at step 0.\n",
      "Episode 139/5000, Total Reward: 0.0000\n",
      "Episode 139 terminated early at step 0.\n",
      "Episode 140/5000, Total Reward: 0.0000\n",
      "Episode 140 terminated early at step 0.\n",
      "Episode 141/5000, Total Reward: 0.0000\n",
      "Episode 141 terminated early at step 0.\n",
      "Episode 142/5000, Total Reward: 0.0000\n",
      "Episode 142 terminated early at step 0.\n",
      "Episode 143/5000, Total Reward: 0.0000\n",
      "Episode 143 terminated early at step 0.\n",
      "Episode 144/5000, Total Reward: 0.0000\n",
      "Episode 144 terminated early at step 0.\n",
      "Episode 145/5000, Total Reward: 0.0000\n",
      "Episode 145 terminated early at step 0.\n",
      "Episode 146/5000, Total Reward: 0.0000\n",
      "Episode 146 terminated early at step 0.\n",
      "Episode 147/5000, Total Reward: 0.0000\n",
      "Episode 147 terminated early at step 0.\n",
      "Episode 148/5000, Total Reward: 0.0000\n",
      "Episode 148 terminated early at step 0.\n",
      "Episode 149/5000, Total Reward: 0.0000\n",
      "Episode 149 terminated early at step 0.\n",
      "Episode 150/5000, Total Reward: 0.0000\n",
      "Episode 150 terminated early at step 0.\n",
      "Episode 151/5000, Total Reward: 0.0000\n",
      "Episode 151 terminated early at step 0.\n",
      "Episode 152/5000, Total Reward: 0.0000\n",
      "Episode 152 terminated early at step 0.\n",
      "Episode 153/5000, Total Reward: 0.0000\n",
      "Episode 153 terminated early at step 0.\n",
      "Episode 154/5000, Total Reward: 0.0000\n",
      "Episode 154 terminated early at step 0.\n",
      "Episode 155/5000, Total Reward: 0.0000\n",
      "Episode 155 terminated early at step 0.\n",
      "Episode 156/5000, Total Reward: 0.0000\n",
      "Episode 156 terminated early at step 0.\n",
      "Episode 157/5000, Total Reward: 0.0000\n",
      "Episode 157 terminated early at step 0.\n",
      "Episode 158/5000, Total Reward: 0.0000\n",
      "Episode 158 terminated early at step 0.\n",
      "Episode 159/5000, Total Reward: 0.0000\n",
      "Episode 159 terminated early at step 0.\n",
      "Episode 160/5000, Total Reward: 0.0000\n",
      "Episode 160 terminated early at step 0.\n",
      "Episode 161/5000, Total Reward: 0.0000\n",
      "Episode 161 terminated early at step 0.\n",
      "Episode 162/5000, Total Reward: 0.0000\n",
      "Episode 162 terminated early at step 0.\n",
      "Episode 163/5000, Total Reward: 0.0000\n",
      "Episode 163 terminated early at step 0.\n",
      "Episode 164/5000, Total Reward: 0.0000\n",
      "Episode 164 terminated early at step 0.\n",
      "Episode 165/5000, Total Reward: 0.0000\n",
      "Episode 165 terminated early at step 0.\n",
      "Episode 166/5000, Total Reward: 0.0000\n",
      "Episode 166 terminated early at step 0.\n",
      "Episode 167/5000, Total Reward: 0.0000\n",
      "Episode 167 terminated early at step 0.\n",
      "Episode 168/5000, Total Reward: 0.0000\n",
      "Episode 168 terminated early at step 0.\n",
      "Episode 169/5000, Total Reward: 0.0000\n",
      "Episode 169 terminated early at step 0.\n",
      "Episode 170/5000, Total Reward: 0.0000\n",
      "Episode 170 terminated early at step 0.\n",
      "Episode 171/5000, Total Reward: 0.0000\n",
      "Episode 171 terminated early at step 0.\n",
      "Episode 172/5000, Total Reward: 0.0000\n",
      "Episode 172 terminated early at step 0.\n",
      "Episode 173/5000, Total Reward: 0.0000\n",
      "Episode 173 terminated early at step 0.\n",
      "Episode 174/5000, Total Reward: 0.0000\n",
      "Episode 174 terminated early at step 0.\n",
      "Episode 175/5000, Total Reward: 0.0000\n",
      "Episode 175 terminated early at step 0.\n",
      "Episode 176/5000, Total Reward: 0.0000\n",
      "Episode 176 terminated early at step 0.\n",
      "Episode 177/5000, Total Reward: 0.0000\n",
      "Episode 177 terminated early at step 0.\n",
      "Episode 178/5000, Total Reward: 0.0000\n",
      "Episode 178 terminated early at step 0.\n",
      "Episode 179/5000, Total Reward: 0.0000\n",
      "Episode 179 terminated early at step 0.\n",
      "Episode 180/5000, Total Reward: 0.0000\n",
      "Episode 180 terminated early at step 0.\n",
      "Episode 181/5000, Total Reward: 0.0000\n",
      "Episode 181 terminated early at step 0.\n",
      "Episode 182/5000, Total Reward: 0.0000\n",
      "Episode 182 terminated early at step 0.\n",
      "Episode 183/5000, Total Reward: 0.0000\n",
      "Episode 183 terminated early at step 0.\n",
      "Episode 184/5000, Total Reward: 0.0000\n",
      "Episode 184 terminated early at step 0.\n",
      "Episode 185/5000, Total Reward: 0.0000\n",
      "Episode 185 terminated early at step 0.\n",
      "Episode 186/5000, Total Reward: 0.0000\n",
      "Episode 186 terminated early at step 0.\n",
      "Episode 187/5000, Total Reward: 0.0000\n",
      "Episode 187 terminated early at step 0.\n",
      "Episode 188/5000, Total Reward: 0.0000\n",
      "Episode 188 terminated early at step 0.\n",
      "Episode 189/5000, Total Reward: 0.0000\n",
      "Episode 189 terminated early at step 0.\n",
      "Episode 190/5000, Total Reward: 0.0000\n",
      "Episode 190 terminated early at step 0.\n",
      "Episode 191/5000, Total Reward: 0.0000\n",
      "Episode 191 terminated early at step 0.\n",
      "Episode 192/5000, Total Reward: 0.0000\n",
      "Episode 192 terminated early at step 0.\n",
      "Episode 193/5000, Total Reward: 0.0000\n",
      "Episode 193 terminated early at step 0.\n",
      "Episode 194/5000, Total Reward: 0.0000\n",
      "Episode 194 terminated early at step 0.\n",
      "Episode 195/5000, Total Reward: 0.0000\n",
      "Episode 195 terminated early at step 0.\n",
      "Episode 196/5000, Total Reward: 0.0000\n",
      "Episode 196 terminated early at step 0.\n",
      "Episode 197/5000, Total Reward: 0.0000\n",
      "Episode 197 terminated early at step 0.\n",
      "Episode 198/5000, Total Reward: 0.0000\n",
      "Episode 198 terminated early at step 0.\n",
      "Episode 199/5000, Total Reward: 0.0000\n",
      "Episode 199 terminated early at step 0.\n",
      "Episode 200/5000, Total Reward: 0.0000\n",
      "Episode 200 terminated early at step 0.\n",
      "Episode 201/5000, Total Reward: 0.0000\n",
      "Episode 201 terminated early at step 0.\n",
      "Episode 202/5000, Total Reward: 0.0000\n",
      "Episode 202 terminated early at step 0.\n",
      "Episode 203/5000, Total Reward: 0.0000\n",
      "Episode 203 terminated early at step 0.\n",
      "Episode 204/5000, Total Reward: 0.0000\n",
      "Episode 204 terminated early at step 0.\n",
      "Episode 205/5000, Total Reward: 0.0000\n",
      "Episode 205 terminated early at step 0.\n",
      "Episode 206/5000, Total Reward: 0.0000\n",
      "Episode 206 terminated early at step 0.\n",
      "Episode 207/5000, Total Reward: 0.0000\n",
      "Episode 207 terminated early at step 0.\n",
      "Episode 208/5000, Total Reward: 0.0000\n",
      "Episode 208 terminated early at step 0.\n",
      "Episode 209/5000, Total Reward: 0.0000\n",
      "Episode 209 terminated early at step 0.\n",
      "Episode 210/5000, Total Reward: 0.0000\n",
      "Episode 210 terminated early at step 0.\n",
      "Episode 211/5000, Total Reward: 0.0000\n",
      "Episode 211 terminated early at step 0.\n",
      "Episode 212/5000, Total Reward: 0.0000\n",
      "Episode 212 terminated early at step 0.\n",
      "Episode 213/5000, Total Reward: 0.0000\n",
      "Episode 213 terminated early at step 0.\n",
      "Episode 214/5000, Total Reward: 0.0000\n",
      "Episode 214 terminated early at step 0.\n",
      "Episode 215/5000, Total Reward: 0.0000\n",
      "Episode 215 terminated early at step 0.\n",
      "Episode 216/5000, Total Reward: 0.0000\n",
      "Episode 216 terminated early at step 0.\n",
      "Episode 217/5000, Total Reward: 0.0000\n",
      "Episode 217 terminated early at step 0.\n",
      "Episode 218/5000, Total Reward: 0.0000\n",
      "Episode 218 terminated early at step 0.\n",
      "Episode 219/5000, Total Reward: 0.0000\n",
      "Episode 219 terminated early at step 0.\n",
      "Episode 220/5000, Total Reward: 0.0000\n",
      "Episode 220 terminated early at step 0.\n",
      "Episode 221/5000, Total Reward: 0.0000\n",
      "Episode 221 terminated early at step 0.\n",
      "Episode 222/5000, Total Reward: 0.0000\n",
      "Episode 222 terminated early at step 0.\n",
      "Episode 223/5000, Total Reward: 0.0000\n",
      "Episode 223 terminated early at step 0.\n",
      "Episode 224/5000, Total Reward: 0.0000\n",
      "Episode 224 terminated early at step 0.\n",
      "Episode 225/5000, Total Reward: 0.0000\n",
      "Episode 225 terminated early at step 0.\n",
      "Episode 226/5000, Total Reward: 0.0000\n",
      "Episode 226 terminated early at step 0.\n",
      "Episode 227/5000, Total Reward: 0.0000\n",
      "Episode 227 terminated early at step 0.\n",
      "Episode 228/5000, Total Reward: 0.0000\n",
      "Episode 228 terminated early at step 0.\n",
      "Episode 229/5000, Total Reward: 0.0000\n",
      "Episode 229 terminated early at step 0.\n",
      "Episode 230/5000, Total Reward: 0.0000\n",
      "Episode 230 terminated early at step 0.\n",
      "Episode 231/5000, Total Reward: 0.0000\n",
      "Episode 231 terminated early at step 0.\n",
      "Episode 232/5000, Total Reward: 0.0000\n",
      "Episode 232 terminated early at step 0.\n",
      "Episode 233/5000, Total Reward: 0.0000\n",
      "Episode 233 terminated early at step 0.\n",
      "Episode 234/5000, Total Reward: 0.0000\n",
      "Episode 234 terminated early at step 0.\n",
      "Episode 235/5000, Total Reward: 0.0000\n",
      "Episode 235 terminated early at step 0.\n",
      "Episode 236/5000, Total Reward: 0.0000\n",
      "Episode 236 terminated early at step 0.\n",
      "Episode 237/5000, Total Reward: 0.0000\n",
      "Episode 237 terminated early at step 0.\n",
      "Episode 238/5000, Total Reward: 0.0000\n",
      "Episode 238 terminated early at step 0.\n",
      "Episode 239/5000, Total Reward: 0.0000\n",
      "Episode 239 terminated early at step 0.\n",
      "Episode 240/5000, Total Reward: 0.0000\n",
      "Episode 240 terminated early at step 0.\n",
      "Episode 241/5000, Total Reward: 0.0000\n",
      "Episode 241 terminated early at step 0.\n",
      "Episode 242/5000, Total Reward: 0.0000\n",
      "Episode 242 terminated early at step 0.\n",
      "Episode 243/5000, Total Reward: 0.0000\n",
      "Episode 243 terminated early at step 0.\n",
      "Episode 244/5000, Total Reward: 0.0000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 224\u001b[0m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModello Actor-Critic RNN salvato correttamente.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 224\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 176\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    173\u001b[0m mujoco\u001b[38;5;241m.\u001b[39mmjr_readPixels(rgb_buffer, depth_buffer, viewport, context)\n\u001b[1;32m    175\u001b[0m \u001b[38;5;66;03m# Salva il frame corrente come array\u001b[39;00m\n\u001b[0;32m--> 176\u001b[0m frame_array \u001b[38;5;241m=\u001b[39m \u001b[43msave_image_to_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrgb_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtobytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviewport_width\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mviewport_height\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    178\u001b[0m frame \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(frame_array)\u001b[38;5;241m.\u001b[39mfloat() \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m255.0\u001b[39m\n\u001b[1;32m    179\u001b[0m frame \u001b[38;5;241m=\u001b[39m frame\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[2], line 14\u001b[0m, in \u001b[0;36msave_image_to_array\u001b[0;34m(rgb_buffer, width, height, target_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(img)\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Ridimensiona l'immagine\u001b[39;00m\n\u001b[0;32m---> 14\u001b[0m img_resized \u001b[38;5;241m=\u001b[39m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mLANCZOS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# Converti l'immagine ridimensionata in un array numpy\u001b[39;00m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(img_resized)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/mujoco-env/lib/python3.9/site-packages/PIL/Image.py:2328\u001b[0m, in \u001b[0;36mImage.resize\u001b[0;34m(self, size, resample, box, reducing_gap)\u001b[0m\n\u001b[1;32m   2316\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2317\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce(factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2318\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreduce)\n\u001b[1;32m   2319\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m Image\u001b[38;5;241m.\u001b[39mreduce(\u001b[38;5;28mself\u001b[39m, factor, box\u001b[38;5;241m=\u001b[39mreduce_box)\n\u001b[1;32m   2320\u001b[0m         )\n\u001b[1;32m   2321\u001b[0m         box \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2322\u001b[0m             (box[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2323\u001b[0m             (box[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2324\u001b[0m             (box[\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_x,\n\u001b[1;32m   2325\u001b[0m             (box[\u001b[38;5;241m3\u001b[39m] \u001b[38;5;241m-\u001b[39m reduce_box[\u001b[38;5;241m1\u001b[39m]) \u001b[38;5;241m/\u001b[39m factor_y,\n\u001b[1;32m   2326\u001b[0m         )\n\u001b[0;32m-> 2328\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_new(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mresample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbox\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "XML_FILE_PATH = \"/Users/edoardozappia/Desktop/Tesi_Magistrale/irregular_shape_2D.xml\"\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "def save_image_to_array(rgb_buffer, width, height, target_size):\n",
    "    \"\"\"\n",
    "    Salva un'immagine dalla simulazione in un array ridimensionato alla dimensione target_size.\n",
    "    \"\"\"\n",
    "    # Converti il buffer in un'immagine\n",
    "    img = np.frombuffer(rgb_buffer, dtype=np.uint8).reshape(height, width, 3)\n",
    "    img = Image.fromarray(img)\n",
    "\n",
    "    # Ridimensiona l'immagine\n",
    "    img_resized = img.resize(target_size, Image.LANCZOS)\n",
    "\n",
    "    # Converti l'immagine ridimensionata in un array numpy\n",
    "    return np.array(img_resized)\n",
    "\n",
    "def visualize_frames(real_frame, predicted_frame, frame_count):\n",
    "    \"\"\"\n",
    "    Visualizza i frame reali e predetti.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Frame reale\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(real_frame)\n",
    "    plt.title(f\"Real Frame - {frame_count}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Frame predetto\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(predicted_frame)\n",
    "    plt.title(f\"Predicted Frame - {frame_count}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Parametri del movimento sinusoidale\n",
    "    freq_x = 0.5  # Frequenza della sinusoide lungo x\n",
    "    freq_y = 0.3  # Frequenza della sinusoide lungo y\n",
    "    freq_phi = 0.1  # Frequenza della sinusoide per la rotazione\n",
    "    amp_x = 5000.0  # Ampiezza della sinusoide lungo x\n",
    "    amp_y = 5000.0  # Ampiezza della sinusoide lungo y\n",
    "    amp_phi = 1.0  # Ampiezza della sinusoide per la rotazione\n",
    "    R = 0.2  # Raggio massimo della circonferenza\n",
    "    dt = 0.01  # Timestep fisso\n",
    "    noise = 0.0  # Rumore bianco gaussiano\n",
    "\n",
    "    # Parametri del modello\n",
    "    latent_dim = 8192  # Modifica in base al tuo modello\n",
    "    action_dim = 8192  # Modifica in base al tuo modello\n",
    "\n",
    "    # Inizializza il modello\n",
    "    actor_critic_rnn = ActorCriticRNN(latent_dim, action_dim)\n",
    "\n",
    "    # Encoder pre-addestrato\n",
    "    encoder = Encoder()\n",
    "    encoder.load_state_dict(torch.load('encoder.pth'))\n",
    "    encoder.eval()\n",
    "\n",
    "    # Decoder pre-addestrato\n",
    "    decoder = Decoder()\n",
    "    decoder.load_state_dict(torch.load('decoder.pth'))\n",
    "    decoder.eval()  \n",
    "\n",
    "    if not glfw.init():\n",
    "        print(\"Impossibile inizializzare GLFW\")\n",
    "        return\n",
    "\n",
    "    glfw.window_hint(glfw.VISIBLE, glfw.TRUE)\n",
    "    window = glfw.create_window(800, 800, \"MuJoCo Viewer\", None, None)\n",
    "    if not window:\n",
    "        print(\"Errore durante la creazione della finestra GLFW\")\n",
    "        glfw.terminate()\n",
    "        return\n",
    "\n",
    "    glfw.make_context_current(window)\n",
    "\n",
    "    model = mujoco.MjModel.from_xml_path(XML_FILE_PATH)\n",
    "    data = mujoco.MjData(model)\n",
    "\n",
    "    # Configura la telecamera\n",
    "    camera = mujoco.MjvCamera()\n",
    "    camera.type = mujoco.mjtCamera.mjCAMERA_FREE\n",
    "    camera.lookat = np.array([0, 0, 0])\n",
    "    camera.distance = 2.0\n",
    "    camera.azimuth = 90\n",
    "    camera.elevation = -90\n",
    "\n",
    "    # Renderer per visualizzare la finestra\n",
    "    scene = mujoco.MjvScene(model, maxgeom=1000)\n",
    "    context = mujoco.MjrContext(model, mujoco.mjtFontScale.mjFONTSCALE_150)\n",
    "\n",
    "    t = 0  # Tempo iniziale\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    # Ottieni dimensioni del viewport\n",
    "    viewport_width, viewport_height = glfw.get_framebuffer_size(window)\n",
    "\n",
    "    # Prealloca buffer per RGB e profondità\n",
    "    rgb_buffer = np.zeros((viewport_height, viewport_width, 3), dtype=np.uint8)\n",
    "    depth_buffer = np.zeros((viewport_height, viewport_width), dtype=np.float32)\n",
    "\n",
    "    k = 0\n",
    "    max_steps = 100000\n",
    "    max_episodes = 5000\n",
    "    episode_termination_threshold = 10.0\n",
    "    counter_threshold_decr = 500\n",
    "\n",
    "    optimizer = optim.Adam(actor_critic_rnn.parameters(), lr=1e-3)\n",
    "\n",
    "    #while not glfw.window_should_close(window):\n",
    "    \n",
    "    fixed_movement = True # per avere stesso movimento all'inizio di ogni episodio\n",
    "\n",
    "    for episode in range(max_episodes):\n",
    "     if fixed_movement:\n",
    "        t = 0  # Tempo iniziale fisso per movimento ripetibile\n",
    "     else:\n",
    "        t = np.random.uniform(0, 2 * np.pi)  # Tempo iniziale casuale per variazioni\n",
    "\n",
    "     if counter_threshold_decr == episode:\n",
    "        episode_termination_threshold -= 0.04\n",
    "        counter_threshold_decr += 250\n",
    "\n",
    "     mujoco.mj_resetData(model, data)\n",
    "\n",
    "     viewport = mujoco.MjrRect(0, 0, viewport_width, viewport_height)\n",
    "     mujoco.mjr_readPixels(rgb_buffer, depth_buffer, viewport, context)\n",
    "     frame_array = save_image_to_array(rgb_buffer.tobytes(), viewport_width, viewport_height, target_size=(64, 64))\n",
    "\n",
    "     frame = torch.tensor(frame_array).float() / 255.0\n",
    "     frame = frame.permute(2, 0, 1)\n",
    "     frame = frame.unsqueeze(0)\n",
    "     with torch.no_grad():\n",
    "        latent_state = encoder(frame)\n",
    "        latent_state = latent_state.view(latent_state.size(0), -1).unsqueeze(1)\n",
    "\n",
    "     episode_reward = 0.0\n",
    "     for step in range(max_steps):\n",
    "        \n",
    "        # Aggiorna il tempo\n",
    "        t += dt\n",
    "\n",
    "        # Forze sinusoidali\n",
    "        force_x = amp_x * np.sin(2 * np.pi * freq_x * t) + noise * np.random.randn()\n",
    "        force_y = amp_y * np.cos(2 * np.pi * freq_y * t) + noise * np.random.randn()\n",
    "\n",
    "        # Momento sinusoidale\n",
    "        torque_phi = amp_phi * np.sin(2 * np.pi * freq_phi * t) + noise * np.random.randn()\n",
    "\n",
    "        # Controlla che l'oggetto rimanga dentro la circonferenza\n",
    "        distance = np.sqrt(data.qpos[0] ** 2 + data.qpos[1] ** 2)\n",
    "        if distance >= R:\n",
    "            # Forza di rimbalzo verso il centro\n",
    "            direction_x = -data.qpos[0] / distance\n",
    "            direction_y = -data.qpos[1] / distance\n",
    "            force_x += direction_x * 1000.0\n",
    "            force_y += direction_y * 1000.0\n",
    "\n",
    "        # Applica le forze e i momenti\n",
    "        data.qfrc_applied[0] = force_x\n",
    "        data.qfrc_applied[1] = force_y\n",
    "        data.qfrc_applied[2] = torque_phi\n",
    "\n",
    "        # Avanza la simulazione\n",
    "        mujoco.mj_step(model, data)\n",
    "\n",
    "        # Renderizza la scena\n",
    "        mujoco.mjv_updateScene(model, data, mujoco.MjvOption(), None, camera, mujoco.mjtCatBit.mjCAT_ALL, scene)\n",
    "        viewport = mujoco.MjrRect(0, 0, viewport_width, viewport_height)\n",
    "        mujoco.mjr_render(viewport, scene, context)\n",
    "\n",
    "        # Leggi i pixel RGB\n",
    "        mujoco.mjr_readPixels(rgb_buffer, depth_buffer, viewport, context)\n",
    "\n",
    "        # Salva il frame corrente come array\n",
    "        frame_array = save_image_to_array(rgb_buffer.tobytes(), viewport_width, viewport_height, target_size=(64, 64))\n",
    "\n",
    "        frame = torch.tensor(frame_array).float() / 255.0\n",
    "        frame = frame.permute(2, 0, 1)\n",
    "        frame = frame.unsqueeze(0)\n",
    "\n",
    "        # Ottieni lo stato latente\n",
    "        with torch.no_grad():\n",
    "            real_next_latent_frame = encoder(frame)\n",
    "            real_next_latent_state = real_next_latent_frame.view(real_next_latent_frame.size(0), -1).unsqueeze(1)\n",
    "\n",
    "        action, _ = actor_critic_rnn(latent_state.clone().detach())\n",
    "\n",
    "        predicted_next_state = latent_state.clone().detach() + action\n",
    "\n",
    "        # Termina l'episodio se lo stato predetto diverge troppo dal reale\n",
    "        if torch.norm(predicted_next_state.detach() - real_next_latent_state.detach()).item() > episode_termination_threshold:\n",
    "            print(f\"Episode {episode} terminated early at step {step}.\")\n",
    "            break\n",
    "\n",
    "        # Aggiorna il modello\n",
    "        loss, actor_loss, critic_loss, reward = update(\n",
    "            actor_critic_rnn, optimizer, latent_state, real_next_latent_state, gamma=0.99\n",
    "        )\n",
    "\n",
    "        episode_reward += reward\n",
    "\n",
    "        print(f\"Step {step}, Reward: {reward:.4f}\")\n",
    "\n",
    "        # Imposta lo stato successivo\n",
    "        latent_state = predicted_next_state.clone().detach()\n",
    "        \n",
    "        glfw.swap_buffers(window)\n",
    "        glfw.poll_events()\n",
    "        time.sleep(dt)\n",
    "     \n",
    "     norm_coeff = step if step > 0 else 1\n",
    "\n",
    "     print(f\"Episode {episode + 1}/{max_episodes}, Mean Reward: {episode_reward/step:.4f}\")\n",
    "\n",
    "    glfw.destroy_window(window)\n",
    "    glfw.terminate()\n",
    "\n",
    "    # Salva i pesi del modello Actor-Critic RNN\n",
    "    torch.save(actor_critic_rnn.state_dict(), \"actor_critic_rnn.pth\")\n",
    "\n",
    "    print(\"Modello Actor-Critic RNN salvato correttamente.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/72/24j7jy9s6sb2xm_dc84kdhm00000gn/T/ipykernel_48134/2926913652.py:169: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  real_frame_decoded = torch.tensor(latent_state).float()\n",
      "/var/folders/72/24j7jy9s6sb2xm_dc84kdhm00000gn/T/ipykernel_48134/2926913652.py:191: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action_image = torch.tensor(action).float()\n",
      "/var/folders/72/24j7jy9s6sb2xm_dc84kdhm00000gn/T/ipykernel_48134/2926913652.py:197: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  predicted_frame_decoded = torch.tensor(predicted_state).float()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azioni uguali: 1\n",
      "Norma azione: 9.314242362976074\n",
      "Differenza stato predetto e stato reale: 9.357549667358398\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.29541015625\n",
      "Differenza stato predetto e stato reale: 9.32781982421875\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.237226486206055\n",
      "Differenza stato predetto e stato reale: 9.253464698791504\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.208675384521484\n",
      "Differenza stato predetto e stato reale: 9.2437105178833\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.178071975708008\n",
      "Differenza stato predetto e stato reale: 9.222015380859375\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.144834518432617\n",
      "Differenza stato predetto e stato reale: 9.185683250427246\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.099295616149902\n",
      "Differenza stato predetto e stato reale: 9.117992401123047\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.112372398376465\n",
      "Differenza stato predetto e stato reale: 9.144478797912598\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.15002155303955\n",
      "Differenza stato predetto e stato reale: 9.180731773376465\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.142354965209961\n",
      "Differenza stato predetto e stato reale: 9.180242538452148\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.108095169067383\n",
      "Differenza stato predetto e stato reale: 9.128707885742188\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.082175254821777\n",
      "Differenza stato predetto e stato reale: 9.10900592803955\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.04670524597168\n",
      "Differenza stato predetto e stato reale: 9.07580852508545\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.974859237670898\n",
      "Differenza stato predetto e stato reale: 9.01439380645752\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.909893035888672\n",
      "Differenza stato predetto e stato reale: 8.94969654083252\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.87240219116211\n",
      "Differenza stato predetto e stato reale: 8.913090705871582\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.809429168701172\n",
      "Differenza stato predetto e stato reale: 8.838001251220703\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.814018249511719\n",
      "Differenza stato predetto e stato reale: 8.857339859008789\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.822887420654297\n",
      "Differenza stato predetto e stato reale: 8.858659744262695\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.809326171875\n",
      "Differenza stato predetto e stato reale: 8.855436325073242\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.809151649475098\n",
      "Differenza stato predetto e stato reale: 8.846283912658691\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.820489883422852\n",
      "Differenza stato predetto e stato reale: 8.862213134765625\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.83657169342041\n",
      "Differenza stato predetto e stato reale: 8.875972747802734\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.874014854431152\n",
      "Differenza stato predetto e stato reale: 8.89295768737793\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.868700981140137\n",
      "Differenza stato predetto e stato reale: 8.902801513671875\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.88811206817627\n",
      "Differenza stato predetto e stato reale: 8.918211936950684\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.909042358398438\n",
      "Differenza stato predetto e stato reale: 8.930673599243164\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.926058769226074\n",
      "Differenza stato predetto e stato reale: 8.965201377868652\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.989994049072266\n",
      "Differenza stato predetto e stato reale: 8.999987602233887\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.99561882019043\n",
      "Differenza stato predetto e stato reale: 9.014876365661621\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.982081413269043\n",
      "Differenza stato predetto e stato reale: 9.006817817687988\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.921628952026367\n",
      "Differenza stato predetto e stato reale: 8.954840660095215\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.858028411865234\n",
      "Differenza stato predetto e stato reale: 8.883332252502441\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.830215454101562\n",
      "Differenza stato predetto e stato reale: 8.868330955505371\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.824605941772461\n",
      "Differenza stato predetto e stato reale: 8.846526145935059\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.826309204101562\n",
      "Differenza stato predetto e stato reale: 8.846776962280273\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.851068496704102\n",
      "Differenza stato predetto e stato reale: 8.874271392822266\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.896536827087402\n",
      "Differenza stato predetto e stato reale: 8.917476654052734\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.916998863220215\n",
      "Differenza stato predetto e stato reale: 8.943188667297363\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.919096946716309\n",
      "Differenza stato predetto e stato reale: 8.962563514709473\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.91493034362793\n",
      "Differenza stato predetto e stato reale: 8.965840339660645\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.883848190307617\n",
      "Differenza stato predetto e stato reale: 8.923356056213379\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.905558586120605\n",
      "Differenza stato predetto e stato reale: 8.934086799621582\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.951509475708008\n",
      "Differenza stato predetto e stato reale: 8.986801147460938\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.07314395904541\n",
      "Differenza stato predetto e stato reale: 9.1035795211792\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.16245174407959\n",
      "Differenza stato predetto e stato reale: 9.185223579406738\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.222237586975098\n",
      "Differenza stato predetto e stato reale: 9.254568099975586\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.260305404663086\n",
      "Differenza stato predetto e stato reale: 9.292845726013184\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.29131031036377\n",
      "Differenza stato predetto e stato reale: 9.325898170471191\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.278151512145996\n",
      "Differenza stato predetto e stato reale: 9.316937446594238\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.214935302734375\n",
      "Differenza stato predetto e stato reale: 9.251947402954102\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.153776168823242\n",
      "Differenza stato predetto e stato reale: 9.16699504852295\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.119316101074219\n",
      "Differenza stato predetto e stato reale: 9.138389587402344\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.103381156921387\n",
      "Differenza stato predetto e stato reale: 9.1199369430542\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.052961349487305\n",
      "Differenza stato predetto e stato reale: 9.070183753967285\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.0066556930542\n",
      "Differenza stato predetto e stato reale: 9.032837867736816\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.967339515686035\n",
      "Differenza stato predetto e stato reale: 8.978822708129883\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.961576461791992\n",
      "Differenza stato predetto e stato reale: 8.976911544799805\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.959217071533203\n",
      "Differenza stato predetto e stato reale: 8.983903884887695\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.982933044433594\n",
      "Differenza stato predetto e stato reale: 9.013409614562988\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.033210754394531\n",
      "Differenza stato predetto e stato reale: 9.056472778320312\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.042637825012207\n",
      "Differenza stato predetto e stato reale: 9.052474975585938\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.042098999023438\n",
      "Differenza stato predetto e stato reale: 9.052689552307129\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.055542945861816\n",
      "Differenza stato predetto e stato reale: 9.06562328338623\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.068329811096191\n",
      "Differenza stato predetto e stato reale: 9.081833839416504\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.064321517944336\n",
      "Differenza stato predetto e stato reale: 9.067082405090332\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.04522705078125\n",
      "Differenza stato predetto e stato reale: 9.059178352355957\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.041893005371094\n",
      "Differenza stato predetto e stato reale: 9.053049087524414\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.043201446533203\n",
      "Differenza stato predetto e stato reale: 9.050973892211914\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.016807556152344\n",
      "Differenza stato predetto e stato reale: 9.01868724822998\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.995978355407715\n",
      "Differenza stato predetto e stato reale: 8.997750282287598\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.973875045776367\n",
      "Differenza stato predetto e stato reale: 8.977896690368652\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.95760726928711\n",
      "Differenza stato predetto e stato reale: 8.965807914733887\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.934114456176758\n",
      "Differenza stato predetto e stato reale: 8.941761016845703\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.93496322631836\n",
      "Differenza stato predetto e stato reale: 8.94525146484375\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.937901496887207\n",
      "Differenza stato predetto e stato reale: 8.948545455932617\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.940678596496582\n",
      "Differenza stato predetto e stato reale: 8.9513578414917\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.934671401977539\n",
      "Differenza stato predetto e stato reale: 8.94469165802002\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.929784774780273\n",
      "Differenza stato predetto e stato reale: 8.942357063293457\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.948830604553223\n",
      "Differenza stato predetto e stato reale: 8.955968856811523\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.980674743652344\n",
      "Differenza stato predetto e stato reale: 8.987356185913086\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.0071439743042\n",
      "Differenza stato predetto e stato reale: 9.013751983642578\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.042527198791504\n",
      "Differenza stato predetto e stato reale: 9.05280876159668\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.081782341003418\n",
      "Differenza stato predetto e stato reale: 9.09168815612793\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.132826805114746\n",
      "Differenza stato predetto e stato reale: 9.135445594787598\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.160503387451172\n",
      "Differenza stato predetto e stato reale: 9.164843559265137\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.191787719726562\n",
      "Differenza stato predetto e stato reale: 9.196221351623535\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.205251693725586\n",
      "Differenza stato predetto e stato reale: 9.21354866027832\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.180537223815918\n",
      "Differenza stato predetto e stato reale: 9.185626029968262\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.171470642089844\n",
      "Differenza stato predetto e stato reale: 9.175442695617676\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.165984153747559\n",
      "Differenza stato predetto e stato reale: 9.169991493225098\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.151372909545898\n",
      "Differenza stato predetto e stato reale: 9.155946731567383\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.14575481414795\n",
      "Differenza stato predetto e stato reale: 9.150870323181152\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.150097846984863\n",
      "Differenza stato predetto e stato reale: 9.155142784118652\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.151969909667969\n",
      "Differenza stato predetto e stato reale: 9.157483100891113\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.155289649963379\n",
      "Differenza stato predetto e stato reale: 9.157618522644043\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.186059951782227\n",
      "Differenza stato predetto e stato reale: 9.19184684753418\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.200993537902832\n",
      "Differenza stato predetto e stato reale: 9.206832885742188\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.207072257995605\n",
      "Differenza stato predetto e stato reale: 9.209868431091309\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.215372085571289\n",
      "Differenza stato predetto e stato reale: 9.221029281616211\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.199238777160645\n",
      "Differenza stato predetto e stato reale: 9.206295013427734\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.154295921325684\n",
      "Differenza stato predetto e stato reale: 9.16120433807373\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.111900329589844\n",
      "Differenza stato predetto e stato reale: 9.120189666748047\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.074060440063477\n",
      "Differenza stato predetto e stato reale: 9.085051536560059\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.027115821838379\n",
      "Differenza stato predetto e stato reale: 9.047067642211914\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.978653907775879\n",
      "Differenza stato predetto e stato reale: 8.997321128845215\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.931544303894043\n",
      "Differenza stato predetto e stato reale: 8.956098556518555\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.875222206115723\n",
      "Differenza stato predetto e stato reale: 8.8930082321167\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.819116592407227\n",
      "Differenza stato predetto e stato reale: 8.844146728515625\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.79171085357666\n",
      "Differenza stato predetto e stato reale: 8.809056282043457\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.766229629516602\n",
      "Differenza stato predetto e stato reale: 8.783553123474121\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.747062683105469\n",
      "Differenza stato predetto e stato reale: 8.776415824890137\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.726176261901855\n",
      "Differenza stato predetto e stato reale: 8.738003730773926\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.718911170959473\n",
      "Differenza stato predetto e stato reale: 8.729626655578613\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.687698364257812\n",
      "Differenza stato predetto e stato reale: 8.698052406311035\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.659077644348145\n",
      "Differenza stato predetto e stato reale: 8.67555046081543\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.647768020629883\n",
      "Differenza stato predetto e stato reale: 8.664684295654297\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.657313346862793\n",
      "Differenza stato predetto e stato reale: 8.6934232711792\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.646615028381348\n",
      "Differenza stato predetto e stato reale: 8.676277160644531\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.697199821472168\n",
      "Differenza stato predetto e stato reale: 8.731193542480469\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.69469928741455\n",
      "Differenza stato predetto e stato reale: 8.737285614013672\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.673622131347656\n",
      "Differenza stato predetto e stato reale: 8.706578254699707\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.684722900390625\n",
      "Differenza stato predetto e stato reale: 8.704118728637695\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.695459365844727\n",
      "Differenza stato predetto e stato reale: 8.712252616882324\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.706212997436523\n",
      "Differenza stato predetto e stato reale: 8.739008903503418\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.732000350952148\n",
      "Differenza stato predetto e stato reale: 8.778937339782715\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.826312065124512\n",
      "Differenza stato predetto e stato reale: 8.871321678161621\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.91710090637207\n",
      "Differenza stato predetto e stato reale: 8.951725006103516\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.979142189025879\n",
      "Differenza stato predetto e stato reale: 9.03622817993164\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.081584930419922\n",
      "Differenza stato predetto e stato reale: 9.164449691772461\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.183348655700684\n",
      "Differenza stato predetto e stato reale: 9.215171813964844\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.249678611755371\n",
      "Differenza stato predetto e stato reale: 9.316730499267578\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.324124336242676\n",
      "Differenza stato predetto e stato reale: 9.374958038330078\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.39595890045166\n",
      "Differenza stato predetto e stato reale: 9.449812889099121\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.39534854888916\n",
      "Differenza stato predetto e stato reale: 9.457541465759277\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.411163330078125\n",
      "Differenza stato predetto e stato reale: 9.48206901550293\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.46753978729248\n",
      "Differenza stato predetto e stato reale: 9.502494812011719\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.547158241271973\n",
      "Differenza stato predetto e stato reale: 9.600317001342773\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.465782165527344\n",
      "Differenza stato predetto e stato reale: 9.52787971496582\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.430047988891602\n",
      "Differenza stato predetto e stato reale: 9.512231826782227\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.443066596984863\n",
      "Differenza stato predetto e stato reale: 9.550440788269043\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.501909255981445\n",
      "Differenza stato predetto e stato reale: 9.608890533447266\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.435797691345215\n",
      "Differenza stato predetto e stato reale: 9.495388984680176\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.365487098693848\n",
      "Differenza stato predetto e stato reale: 9.484518051147461\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.25031566619873\n",
      "Differenza stato predetto e stato reale: 9.370444297790527\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.23226261138916\n",
      "Differenza stato predetto e stato reale: 9.319939613342285\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.296985626220703\n",
      "Differenza stato predetto e stato reale: 9.342035293579102\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.305357933044434\n",
      "Differenza stato predetto e stato reale: 9.403696060180664\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.232527732849121\n",
      "Differenza stato predetto e stato reale: 9.332830429077148\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.181565284729004\n",
      "Differenza stato predetto e stato reale: 9.312034606933594\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.192198753356934\n",
      "Differenza stato predetto e stato reale: 9.302146911621094\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.123071670532227\n",
      "Differenza stato predetto e stato reale: 9.199915885925293\n",
      "Azioni uguali: 1\n",
      "Norma azione: 9.068366050720215\n",
      "Differenza stato predetto e stato reale: 9.209859848022461\n",
      "Azioni uguali: 1\n",
      "Norma azione: 8.972992897033691\n"
     ]
    }
   ],
   "source": [
    "XML_FILE_PATH = \"/Users/edoardozappia/Desktop/Tesi_Magistrale/irregular_shape_2D.xml\"\n",
    "\n",
    "def save_image_to_array(rgb_buffer, width, height, target_size):\n",
    "    \"\"\"\n",
    "    Salva un'immagine dalla simulazione in un array ridimensionato alla dimensione target_size.\n",
    "    \"\"\"\n",
    "    # Converti il buffer in un'immagine\n",
    "    img = np.frombuffer(rgb_buffer, dtype=np.uint8).reshape(height, width, 3)\n",
    "    img = Image.fromarray(img)\n",
    "\n",
    "    # Ridimensiona l'immagine\n",
    "    img_resized = img.resize(target_size, Image.LANCZOS)\n",
    "\n",
    "    # Converti l'immagine ridimensionata in un array numpy\n",
    "    return np.array(img_resized)\n",
    "\n",
    "def visualize_frames(real_frame, predicted_frame, frame_count):\n",
    "    \"\"\"\n",
    "    Visualizza i frame reali e predetti.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "\n",
    "    # Frame reale\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.imshow(real_frame)\n",
    "    plt.title(f\"Real Frame - {frame_count}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Frame predetto\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.imshow(predicted_frame)\n",
    "    plt.title(f\"Predicted Frame - {frame_count}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "def main():\n",
    "    # Parametri del movimento sinusoidale\n",
    "    freq_x = 0.5  # Frequenza della sinusoide lungo x\n",
    "    freq_y = 0.3  # Frequenza della sinusoide lungo y\n",
    "    freq_phi = 0.1  # Frequenza della sinusoide per la rotazione\n",
    "    amp_x = 5000.0  # Ampiezza della sinusoide lungo x\n",
    "    amp_y = 5000.0  # Ampiezza della sinusoide lungo y\n",
    "    amp_phi = 1.0  # Ampiezza della sinusoide per la rotazione\n",
    "    R = 0.2  # Raggio massimo della circonferenza\n",
    "    dt = 0.01  # Timestep fisso\n",
    "    noise = 0.0  # Rumore bianco gaussiano\n",
    "\n",
    "    # Parametri del modello\n",
    "    latent_dim = 8192  # Modifica in base al tuo modello\n",
    "    action_dim = 8192  # Modifica in base al tuo modello\n",
    "\n",
    "    # Inizializza il modello\n",
    "    actor_critic_rnn = ActorCriticRNN(latent_dim, action_dim)\n",
    "\n",
    "    # Carica i pesi salvati\n",
    "    actor_critic_rnn.load_state_dict(torch.load(\"actor_critic_rnn.pth\"))\n",
    "    actor_critic_rnn.eval()  # Imposta il modello in modalità valutazione\n",
    "\n",
    "    # Encoder pre-addestrato\n",
    "    encoder = Encoder()\n",
    "    encoder.load_state_dict(torch.load('encoder.pth'))\n",
    "    encoder.eval()\n",
    "\n",
    "    # Decoder pre-addestrato\n",
    "    decoder = Decoder()\n",
    "    decoder.load_state_dict(torch.load('decoder.pth'))\n",
    "    decoder.eval()  \n",
    "\n",
    "    if not glfw.init():\n",
    "        print(\"Impossibile inizializzare GLFW\")\n",
    "        return\n",
    "\n",
    "    glfw.window_hint(glfw.VISIBLE, glfw.TRUE)\n",
    "    window = glfw.create_window(800, 800, \"MuJoCo Viewer\", None, None)\n",
    "    if not window:\n",
    "        print(\"Errore durante la creazione della finestra GLFW\")\n",
    "        glfw.terminate()\n",
    "        return\n",
    "\n",
    "    glfw.make_context_current(window)\n",
    "\n",
    "    model = mujoco.MjModel.from_xml_path(XML_FILE_PATH)\n",
    "    data = mujoco.MjData(model)\n",
    "\n",
    "    # Configura la telecamera\n",
    "    camera = mujoco.MjvCamera()\n",
    "    camera.type = mujoco.mjtCamera.mjCAMERA_FREE\n",
    "    camera.lookat = np.array([0, 0, 0])\n",
    "    camera.distance = 2.0\n",
    "    camera.azimuth = 90\n",
    "    camera.elevation = -90\n",
    "\n",
    "    # Renderer per visualizzare la finestra\n",
    "    scene = mujoco.MjvScene(model, maxgeom=1000)\n",
    "    context = mujoco.MjrContext(model, mujoco.mjtFontScale.mjFONTSCALE_150)\n",
    "\n",
    "    t = 0  # Tempo iniziale\n",
    "\n",
    "    frame_count = 0\n",
    "\n",
    "    # Ottieni dimensioni del viewport\n",
    "    viewport_width, viewport_height = glfw.get_framebuffer_size(window)\n",
    "\n",
    "    # Prealloca buffer per RGB e profondità\n",
    "    rgb_buffer = np.zeros((viewport_height, viewport_width, 3), dtype=np.uint8)\n",
    "    depth_buffer = np.zeros((viewport_height, viewport_width), dtype=np.float32)\n",
    "\n",
    "\n",
    "    k = 0\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    while not glfw.window_should_close(window):\n",
    "        \n",
    "        # Aggiorna il tempo\n",
    "        t += dt\n",
    "\n",
    "        # Forze sinusoidali\n",
    "        force_x = amp_x * np.sin(2 * np.pi * freq_x * t) + noise * np.random.randn()\n",
    "        force_y = amp_y * np.cos(2 * np.pi * freq_y * t) + noise * np.random.randn()\n",
    "\n",
    "        # Momento sinusoidale\n",
    "        torque_phi = amp_phi * np.sin(2 * np.pi * freq_phi * t) + noise * np.random.randn()\n",
    "\n",
    "        # Controlla che l'oggetto rimanga dentro la circonferenza\n",
    "        distance = np.sqrt(data.qpos[0] ** 2 + data.qpos[1] ** 2)\n",
    "        if distance >= R:\n",
    "            # Forza di rimbalzo verso il centro\n",
    "            direction_x = -data.qpos[0] / distance\n",
    "            direction_y = -data.qpos[1] / distance\n",
    "            force_x += direction_x * 1000.0\n",
    "            force_y += direction_y * 1000.0\n",
    "\n",
    "        # Applica le forze e i momenti\n",
    "        data.qfrc_applied[0] = force_x\n",
    "        data.qfrc_applied[1] = force_y\n",
    "        data.qfrc_applied[2] = torque_phi\n",
    "\n",
    "        # Avanza la simulazione\n",
    "        mujoco.mj_step(model, data)\n",
    "\n",
    "        # Debug: verifica posizione e velocità\n",
    "        #print(f\"Tempo: {t:.2f}, Posizione: x={data.qpos[0]:.3f}, y={data.qpos[1]:.3f}, phi={data.qpos[2]:.3f}\")\n",
    "        #print(f\"Velocità: x_vel={data.qvel[0]:.3f}, y_vel={data.qvel[1]:.3f}, phi_vel={data.qvel[2]:.3f}\")\n",
    "\n",
    "        # Renderizza la scena\n",
    "        mujoco.mjv_updateScene(model, data, mujoco.MjvOption(), None, camera, mujoco.mjtCatBit.mjCAT_ALL, scene)\n",
    "        viewport = mujoco.MjrRect(0, 0, viewport_width, viewport_height)\n",
    "        mujoco.mjr_render(viewport, scene, context)\n",
    "\n",
    "        # Leggi i pixel RGB\n",
    "        mujoco.mjr_readPixels(rgb_buffer, depth_buffer, viewport, context)\n",
    "\n",
    "        # Salva il frame corrente come array\n",
    "        frame_array = save_image_to_array(rgb_buffer.tobytes(), viewport_width, viewport_height, target_size=(64, 64))\n",
    "\n",
    "        frame = torch.tensor(frame_array).float() / 255.0\n",
    "        frame = frame.permute(2, 0, 1)\n",
    "        frame = frame.unsqueeze(0)\n",
    "\n",
    "        # Ottieni lo stato latente\n",
    "        with torch.no_grad():\n",
    "            latent_state = encoder(frame)\n",
    "            latent_state = latent_state.view(latent_state.size(0), -1).unsqueeze(1)\n",
    "        \n",
    "        # Decodifica il frame reale e predetto\n",
    "        real_frame_decoded = torch.tensor(latent_state).float()\n",
    "        real_frame_decoded = real_frame_decoded.view(1, 128, 8, 8)\n",
    "        real_frame_decoded = decoder(real_frame_decoded).detach().squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "        \n",
    "        if frame_count != 0:\n",
    "            # Visualizza i frame reali e predetti\n",
    "            #visualize_frames(real_frame_decoded, predicted_frame_decoded, frame_count)\n",
    "            print(f\"Differenza stato predetto e stato reale: {torch.norm(latent_state - predicted_state)}\")\n",
    "\n",
    "        if frame_count == 0:\n",
    "            action_0, _ = actor_critic_rnn(latent_state)\n",
    "\n",
    "        action, _ = actor_critic_rnn(latent_state)\n",
    "\n",
    "        if torch.norm(action_0 - action) == 0:\n",
    "            k += 1\n",
    "        \n",
    "        print(f\"Azioni uguali: {k}\")\n",
    "        predicted_state = latent_state + action\n",
    "\n",
    "        print(f\"Norma azione: {torch.norm(action)}\")\n",
    "\n",
    "        action_image = torch.tensor(action).float()\n",
    "        action_image = action_image.view(1, 128, 8, 8)\n",
    "        action_image = decoder(action_image).detach().squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        #visualize_frames(action_image, real_frame_decoded, frame_count)\n",
    "\n",
    "        predicted_frame_decoded = torch.tensor(predicted_state).float()\n",
    "        predicted_frame_decoded = predicted_frame_decoded.view(1, 128, 8, 8)\n",
    "        predicted_frame_decoded = decoder(predicted_frame_decoded).detach().squeeze(0).permute(1, 2, 0).cpu().numpy()\n",
    "\n",
    "        frame_count += 1\n",
    "\n",
    "        glfw.swap_buffers(window)\n",
    "        glfw.poll_events()\n",
    "        time.sleep(dt)\n",
    "\n",
    "    glfw.destroy_window(window)\n",
    "    glfw.terminate()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mujoco-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
